# -*- coding: utf-8 -*-
"""RQ3 Simulation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RhcbOJTr0KYnGwTIn8EnuhXlqG4IH6o1
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_theme(style="whitegrid")
sns.set_palette(sns.color_palette("husl", 9))

###############################################################################
# 1. PARAMETER DEFINITIONS & SCENARIO SETUP
###############################################################################

def get_scenario_parameters(scenario="baseline"):
    """
    Returns scenario-specific parameter ranges (min, max)
    for data sources, IoT infrastructure, AI capabilities, etc.
    """
    if scenario == "baseline":
        # Example: moderate coverage, moderate AI, moderate governance
        params = {
            # Data Sources
            "traffic_data_volume": (1000, 2000),   # daily records
            "energy_data_volume": (800, 1500),
            "air_quality_data_volume": (300, 800),
            "waste_data_volume": (400, 900),

            # IoT Infrastructure
            "num_iot_devices": (50, 200),
            "sensor_coverage": (0.3, 0.6),
            "data_transmission_rate": (0.5, 1.0),
            "sensor_reliability": (0.6, 0.9),

            # AI Capabilities
            "ai_data_cleaning_factor": (0.7, 0.9),
            "ai_realtime_analytics": (0.5, 0.8),
            "ai_predictive_models": (0.3, 0.7),

            # Urban Context
            "population_density": (2000, 5000),
            "grid_efficiency": (0.4, 0.6),
            "transport_usage": (0.3, 0.6),
            "env_challenges": (0.4, 0.7),

            # Governance
            "policy_responsiveness": (0.3, 0.5),
            "data_sharing_regulations": (0.2, 0.5),
            "budget_allocation": (100, 300),

            # Tech Constraints
            "storage_capacity": (100, 300),
            "network_bandwidth": (10, 50),
            "latency": (50, 200),
            "hardware_costs": (1000, 3000)
        }
        return params

    elif scenario == "high_investment":
        # Example scenario with more IoT devices, better AI, bigger budget
        params = {
            "traffic_data_volume": (1500, 3000),
            "energy_data_volume": (1200, 2500),
            "air_quality_data_volume": (500, 1200),
            "waste_data_volume": (600, 1300),

            "num_iot_devices": (200, 500),
            "sensor_coverage": (0.5, 0.9),
            "data_transmission_rate": (1.0, 2.0),
            "sensor_reliability": (0.8, 0.95),

            "ai_data_cleaning_factor": (0.8, 0.95),
            "ai_realtime_analytics": (0.7, 0.9),
            "ai_predictive_models": (0.5, 0.8),

            "population_density": (3000, 7000),
            "grid_efficiency": (0.5, 0.8),
            "transport_usage": (0.4, 0.7),
            "env_challenges": (0.5, 0.8),

            "policy_responsiveness": (0.4, 0.7),
            "data_sharing_regulations": (0.3, 0.6),
            "budget_allocation": (300, 600),

            "storage_capacity": (200, 600),
            "network_bandwidth": (30, 100),
            "latency": (30, 100),
            "hardware_costs": (2000, 5000)
        }
        return params

    elif scenario == "low_resource":
        # New scenario with lower budgets, limited AI, etc.
        params = {
            "traffic_data_volume": (500, 1200),
            "energy_data_volume": (400, 900),
            "air_quality_data_volume": (200, 500),
            "waste_data_volume": (300, 700),

            "num_iot_devices": (20, 100),
            "sensor_coverage": (0.2, 0.4),
            "data_transmission_rate": (0.3, 0.6),
            "sensor_reliability": (0.4, 0.7),

            "ai_data_cleaning_factor": (0.5, 0.7),
            "ai_realtime_analytics": (0.3, 0.6),
            "ai_predictive_models": (0.2, 0.5),

            "population_density": (1500, 4000),
            "grid_efficiency": (0.3, 0.5),
            "transport_usage": (0.2, 0.5),
            "env_challenges": (0.5, 0.8),

            "policy_responsiveness": (0.1, 0.3),
            "data_sharing_regulations": (0.1, 0.3),
            "budget_allocation": (50, 200),

            "storage_capacity": (50, 150),
            "network_bandwidth": (5, 20),
            "latency": (100, 300),
            "hardware_costs": (500, 2000)
        }
        return params

    else:
        raise ValueError(f"Unknown scenario: {scenario}")

def sample_param(range_tuple):
    """Utility to sample from a uniform distribution given a (min, max) tuple."""
    return np.random.uniform(*range_tuple)

###############################################################################
# 2. SIMULATION LOGIC
###############################################################################

def run_simulation(time_steps=12, param_dict=None):
    """
    Simulates how AI + IoT can improve data integration, processing accuracy,
    and real-time decision-making in a low-income city context.

    Returns the final step metrics in a dict.
    """
    # Extract parameters from param_dict (a single sample of scenario ranges)
    traffic_data      = param_dict["traffic_data_volume"]
    energy_data       = param_dict["energy_data_volume"]
    airq_data         = param_dict["air_quality_data_volume"]
    waste_data        = param_dict["waste_data_volume"]

    num_iot           = param_dict["num_iot_devices"]
    coverage          = param_dict["sensor_coverage"]
    transmission_rate = param_dict["data_transmission_rate"]
    reliability       = param_dict["sensor_reliability"]

    ai_cleaning       = param_dict["ai_data_cleaning_factor"]
    ai_realtime       = param_dict["ai_realtime_analytics"]
    ai_predictive     = param_dict["ai_predictive_models"]

    pop_density       = param_dict["population_density"]
    grid_eff          = param_dict["grid_efficiency"]
    transport_usage   = param_dict["transport_usage"]
    env_challenges    = param_dict["env_challenges"]

    policy_resp       = param_dict["policy_responsiveness"]
    data_sharing      = param_dict["data_sharing_regulations"]
    budget            = param_dict["budget_allocation"]

    storage           = param_dict["storage_capacity"]
    bandwidth         = param_dict["network_bandwidth"]
    latency           = param_dict["latency"]
    hw_costs          = param_dict["hardware_costs"]

    # Initialize time-series arrays or placeholders for key outputs
    data_integration_list      = []
    scalability_list           = []
    data_accuracy_list         = []
    decision_efficiency_list   = []
    economic_impact_list       = []
    sustainability_list        = []

    # Some intermediate states
    integrated_data_score  = 0.2
    coverage_score         = coverage
    real_time_accuracy     = 0.5
    policy_response_time   = 300  # in minutes, arbitrary baseline
    total_cost_savings     = 0.0
    environment_score      = 0.4  # simplistic representation (0-1)

    for t in range(time_steps):
        # 1) Data Integration
        data_integration_factor = (
            coverage_score
            + ai_cleaning
            + data_sharing
            + (t / time_steps * 0.2)
        ) / 4.0
        integrated_data_score = min(1.0, integrated_data_score + data_integration_factor * 0.05)
        data_integration_list.append(integrated_data_score)

        # 2) Scalability
        iot_success_rate = reliability * (transmission_rate / (1 + latency * 0.001))
        coverage_score = coverage_score + (iot_success_rate * 0.01)
        coverage_score = min(1.0, coverage_score)
        current_scalability = (num_iot * coverage_score * iot_success_rate) / 1000.0
        scalability_list.append(current_scalability)

        # 3) Data Processing Accuracy
        real_time_accuracy += (ai_realtime * 0.02) + (ai_predictive * 0.01)
        real_time_accuracy -= (1 / max(1, bandwidth) * 0.005)
        real_time_accuracy = max(0, min(1.0, real_time_accuracy))
        data_accuracy_list.append(real_time_accuracy)

        # 4) Decision-Making Efficiency
        policy_response_time *= (1 - (policy_resp + real_time_accuracy) * 0.001)
        decision_efficiency_list.append(max(0, 300 - policy_response_time))

        # 5) Economic Impact
        step_savings = integrated_data_score * coverage_score * 10.0
        total_cost_savings += step_savings
        economic_impact_list.append(total_cost_savings)

        # 6) Sustainability
        environment_score += (real_time_accuracy + coverage_score) * 0.01
        environment_score -= env_challenges * 0.001
        environment_score = max(0, min(1.0, environment_score))
        sustainability_list.append(environment_score)

    # Return final step or entire time series
    return {
        "Data_Integration_TS": data_integration_list,
        "Scalability_TS": scalability_list,
        "Data_Accuracy_TS": data_accuracy_list,
        "Decision_Efficiency_TS": decision_efficiency_list,
        "Economic_Impact_TS": economic_impact_list,
        "Sustainability_TS": sustainability_list
    }

###############################################################################
# 3. MONTE CARLO SIMULATION
###############################################################################

def run_monte_carlo(n_runs=50, time_steps=12, scenario="baseline"):
    """
    Execute multiple simulations for a given scenario.
    We'll store only the final step's outputs in a DataFrame for easy correlation or analysis.
    """
    scenario_ranges = get_scenario_parameters(scenario)
    final_results = {
        "Data_Integration_Final": [],
        "Scalability_Final": [],
        "Data_Accuracy_Final": [],
        "Decision_Efficiency_Final": [],
        "Economic_Impact_Final": [],
        "Sustainability_Final": []
    }

    for _ in range(n_runs):
        param_dict = {k: sample_param(v) for k, v in scenario_ranges.items()}
        sim_data = run_simulation(time_steps=time_steps, param_dict=param_dict)

        final_results["Data_Integration_Final"].append(sim_data["Data_Integration_TS"][-1])
        final_results["Scalability_Final"].append(sim_data["Scalability_TS"][-1])
        final_results["Data_Accuracy_Final"].append(sim_data["Data_Accuracy_TS"][-1])
        final_results["Decision_Efficiency_Final"].append(sim_data["Decision_Efficiency_TS"][-1])
        final_results["Economic_Impact_Final"].append(sim_data["Economic_Impact_TS"][-1])
        final_results["Sustainability_Final"].append(sim_data["Sustainability_TS"][-1])

    df_results = pd.DataFrame(final_results)
    return df_results

###############################################################################
# 4. VISUALIZATION & STAKEHOLDER REPORT
###############################################################################

def visualize_time_series(sim_data, title="Simulation Time Series"):
    df = pd.DataFrame({
        "Data_Integration": sim_data["Data_Integration_TS"],
        "Scalability": sim_data["Scalability_TS"],
        "Data_Accuracy": sim_data["Data_Accuracy_TS"],
        "Decision_Efficiency": sim_data["Decision_Efficiency_TS"],
        "Economic_Impact": sim_data["Economic_Impact_TS"],
        "Sustainability": sim_data["Sustainability_TS"]
    })
    df.plot(subplots=True, figsize=(10,12), layout=(3,2), sharex=True)
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

def visualize_final_correlations(df_results, title="Final Step Correlation"):
    corr_matrix = df_results.corr()
    plt.figure(figsize=(8, 6))
    sns.heatmap(corr_matrix, annot=True, cmap="viridis", fmt=".2f")
    plt.title(title)
    plt.tight_layout()
    plt.show()

def stakeholder_report(df_results, scenario_name="Baseline"):
    print(f"\n===== {scenario_name.upper()} SCENARIO - STAKEHOLDER REPORT =====")
    summary_stats = df_results.describe(percentiles=[0.1, 0.5, 0.9])
    print(summary_stats)

    avg_data_integration = df_results["Data_Integration_Final"].mean()
    avg_sustainability   = df_results["Sustainability_Final"].mean()
    avg_econ_impact      = df_results["Economic_Impact_Final"].mean()

    print("\nKey Highlights:")
    print(f" - Average Data Integration Score: {avg_data_integration:.2f}")
    print(f" - Average Sustainability Score: {avg_sustainability:.2f}")
    print(f" - Average Economic Impact: {avg_econ_impact:.2f} (units of $k or scaled)\n")
    print("===============================================================\n")

###############################################################################
# 5. DEMONSTRATION (MAIN)
###############################################################################

if __name__ == "__main__":
    # Single run demonstration for Baseline
    scenario_params = get_scenario_parameters("baseline")
    single_run_param = {k: sample_param(v) for k, v in scenario_params.items()}
    single_run_data = run_simulation(time_steps=12, param_dict=single_run_param)
    visualize_time_series(single_run_data, title="Baseline Single Run")

    # 1) Baseline scenario
    df_baseline = run_monte_carlo(n_runs=50, time_steps=12, scenario="baseline")
    visualize_final_correlations(df_baseline, title="Baseline Final Outputs Correlation")
    stakeholder_report(df_baseline, scenario_name="Baseline")

    # 2) High Investment scenario
    df_high_invest = run_monte_carlo(n_runs=50, time_steps=12, scenario="high_investment")
    visualize_final_correlations(df_high_invest, title="High Investment Final Outputs Correlation")
    stakeholder_report(df_high_invest, scenario_name="High Investment")

    # 3) Low Resource scenario
    df_low_res = run_monte_carlo(n_runs=50, time_steps=12, scenario="low_resource")
    visualize_final_correlations(df_low_res, title="Low Resource Final Outputs Correlation")
    stakeholder_report(df_low_res, scenario_name="Low Resource")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_theme(style="whitegrid")
sns.set_palette(sns.color_palette("husl", 9))

###############################################################################
# SCENARIO DEFINITION
###############################################################################

def get_scenario_parameters(scenario="baseline"):
    """
    Returns scenario-specific parameters for data management in smart cities, including:
    - Correlation matrices, means, stds for correlated draws (e.g., IoT adoption, AI capability)
    - Markov chain transition matrix for 'network outage' shocks
    - Additional cost/benefit or synergy factors relevant to data management
    """
    if scenario == "baseline":
        params = {
            # Correlation matrix for [iot_adoption, ai_capability, data_complexity]
            # E.g., if AI capability is correlated with IoT adoption, etc.
            "corr_matrix": np.array([
                [1.0,  0.3,  0.2],
                [0.3,  1.0,  0.2],
                [0.2,  0.2,  1.0]
            ]),
            # Means and standard deviations
            # For example: iot_adoption ~ 0.5, ai_capability ~ 0.4, data_complexity ~ 0.6
            "means": np.array([0.5, 0.4, 0.6]),
            "stds":  np.array([0.1, 0.1, 0.1]),

            # Additional AI improvement ranges (min, max) for data integration, real-time analytics
            "ai_data_integration_factor": (0.01, 0.03),
            "ai_realtime_analytics_factor": (0.01, 0.03),
            "iot_scalability_boost": (0.01, 0.02),

            # Markov chain transition probabilities for a 'network outage' shock
            # [ [P(no-outage->no-outage), P(no-outage->outage)],
            #   [P(outage->no-outage),    P(outage->outage)] ]
            "shock_transition": np.array([
                [0.90, 0.10],  # 10% chance to go from no-outage to outage
                [0.40, 0.60]   # 40% chance to recover from outage
            ]),
            "shock_impact": 0.2,  # how strongly an outage reduces data integration

            # Cost/Benefit or synergy factors
            "data_storage_cost": 50,        # e.g., $50 per GB
            "analytics_platform_cost": 100, # e.g., base cost for AI platform
            "governance_synergy_factor": 0.5, # synergy from policy & data sharing
            "max_data_integration": 1.0,
            "min_data_integration": 0.0
        }
        return params

    elif scenario == "high_investment":
        # Example scenario with bigger budget, higher AI adoption, and more IoT coverage
        params = {
            "corr_matrix": np.array([
                [1.0,  0.4,  0.3],
                [0.4,  1.0,  0.3],
                [0.3,  0.3,  1.0]
            ]),
            "means": np.array([0.7, 0.6, 0.5]),
            "stds":  np.array([0.1, 0.1, 0.1]),

            "ai_data_integration_factor": (0.02, 0.05),
            "ai_realtime_analytics_factor": (0.02, 0.05),
            "iot_scalability_boost": (0.02, 0.04),

            "shock_transition": np.array([
                [0.92, 0.08],
                [0.30, 0.70]
            ]),
            "shock_impact": 0.15,

            "data_storage_cost": 70,
            "analytics_platform_cost": 200,
            "governance_synergy_factor": 0.6,
            "max_data_integration": 1.0,
            "min_data_integration": 0.0
        }
        return params

    elif scenario == "low_resource":
        # Scenario with smaller budget, more frequent outages, higher data complexity
        params = {
            "corr_matrix": np.array([
                [1.0,  0.2,  0.3],
                [0.2,  1.0,  0.2],
                [0.3,  0.2,  1.0]
            ]),
            "means": np.array([0.3, 0.3, 0.7]),
            "stds":  np.array([0.1, 0.1, 0.1]),

            "ai_data_integration_factor": (0.005, 0.02),
            "ai_realtime_analytics_factor": (0.005, 0.02),
            "iot_scalability_boost": (0.005, 0.015),

            "shock_transition": np.array([
                [0.80, 0.20],
                [0.50, 0.50]
            ]),
            "shock_impact": 0.3,

            "data_storage_cost": 30,
            "analytics_platform_cost": 80,
            "governance_synergy_factor": 0.4,
            "max_data_integration": 1.0,
            "min_data_integration": 0.0
        }
        return params

    else:
        raise ValueError(f"Unknown scenario: {scenario}")

###############################################################################
# HELPER: CORRELATED PARAMETER SAMPLING
###############################################################################

def sample_correlated_params(means, stds, corr_matrix):
    """
    Generate a single sample from a multivariate normal distribution.
    Suppose we have [iot_adoption, ai_capability, data_complexity].
    """
    dim = len(means)
    cov_matrix = np.diag(stds).dot(corr_matrix).dot(np.diag(stds))

    L = np.linalg.cholesky(cov_matrix)
    z = np.random.normal(size=dim)

    correlated = L.dot(z) + means
    return correlated

def sample_correlated_params_each_step(time_steps, means, stds, corr_matrix):
    """
    For temporal variation, generate an array (time_steps, dim)
    from a multivariate normal each step.
    """
    dim = len(means)
    cov_matrix = np.diag(stds).dot(corr_matrix).dot(np.diag(stds))
    L = np.linalg.cholesky(cov_matrix)

    z = np.random.normal(size=(dim, time_steps))
    correlated_series = (L @ z).T + means
    return correlated_series

###############################################################################
# HELPER: MARKOV CHAIN FOR OUTAGES
###############################################################################

def simulate_network_shocks(time_steps, transition_matrix):
    """
    2-state Markov chain for 'no-outage' (0) vs. 'outage' (1).
    """
    states = np.zeros(time_steps, dtype=int)
    for t in range(1, time_steps):
        prev_state = states[t-1]
        if prev_state == 0:
            if np.random.rand() < transition_matrix[0,0]:
                states[t] = 0
            else:
                states[t] = 1
        else:
            if np.random.rand() < transition_matrix[1,0]:
                states[t] = 0
            else:
                states[t] = 1
    return states

###############################################################################
# MAIN SIMULATION
###############################################################################

def run_simulation(
    time_steps=24,
    scenario_params=None,
    # Initial states
    init_data_integration=0.3,   # how integrated the data is initially
    init_iot_scale=0.2,          # how scalable IoT is initially
    init_realtime_accuracy=0.3,  # baseline real-time analytics accuracy
    init_decision_efficiency=0.2 # how much policy decisions rely on real-time data
):
    """
    Simulate how AI & IoT improve data management under a Markov chain for network outages,
    correlated draws for IoT adoption/AI/data complexity, and cost factors for data storage/analytics.
    """

    if scenario_params is None:
        scenario_params = get_scenario_parameters("baseline")

    # 1) Generate time-varying correlated draws
    #    e.g., [iot_adoption(t), ai_capability(t), data_complexity(t)]
    correlated_series = sample_correlated_params_each_step(
        time_steps,
        scenario_params["means"],
        scenario_params["stds"],
        scenario_params["corr_matrix"]
    )

    # 2) Network outage states
    shock_states = simulate_network_shocks(time_steps, scenario_params["shock_transition"])

    # 3) Uniform draws for constant AI factors
    ai_data_integration_factor = np.random.uniform(*scenario_params["ai_data_integration_factor"])
    ai_realtime_factor         = np.random.uniform(*scenario_params["ai_realtime_analytics_factor"])
    iot_boost                  = np.random.uniform(*scenario_params["iot_scalability_boost"])

    # 4) Unpack other scenario parameters
    shock_impact = scenario_params["shock_impact"]
    data_storage_cost = scenario_params["data_storage_cost"]
    analytics_cost    = scenario_params["analytics_platform_cost"]
    synergy_factor    = scenario_params["governance_synergy_factor"]
    max_integration   = scenario_params["max_data_integration"]
    min_integration   = scenario_params["min_data_integration"]

    # Arrays for storing time series
    data_integration_list  = np.zeros(time_steps)
    iot_scalability_list   = np.zeros(time_steps)
    data_accuracy_list     = np.zeros(time_steps)
    decision_eff_list      = np.zeros(time_steps)
    cost_benefit_list      = np.zeros(time_steps)

    # Initialize states
    data_integration = init_data_integration
    iot_scale        = init_iot_scale
    realtime_acc     = init_realtime_accuracy
    decision_eff     = init_decision_efficiency
    cumulative_savings = 0.0

    for t in range(time_steps):
        iot_adoption    = correlated_series[t, 0]
        ai_capability   = correlated_series[t, 1]
        data_complexity = correlated_series[t, 2]

        # Store current state
        data_integration_list[t] = data_integration
        iot_scalability_list[t]  = iot_scale
        data_accuracy_list[t]    = realtime_acc
        decision_eff_list[t]     = decision_eff
        cost_benefit_list[t]     = cumulative_savings

        # 1) Improve data integration
        #    AI + synergy factor -> better data aggregation
        #    Data complexity might reduce effective integration
        integration_boost = ai_data_integration_factor * ai_capability
        synergy_boost     = synergy_factor * 0.01
        data_integration += (integration_boost + synergy_boost - data_complexity * 0.005)
        data_integration  = max(min_integration, min(max_integration, data_integration))

        # 2) IoT Scalability
        #    If iot_adoption is high, we improve iot_scale
        iot_scale += iot_adoption * iot_boost * 0.01
        iot_scale  = max(0, min(1.0, iot_scale))

        # 3) Real-time Data Accuracy
        #    Improved by AI real-time factor, reduced by data complexity
        realtime_acc += (ai_realtime_factor * ai_capability * 0.01) - (data_complexity * 0.001)
        realtime_acc  = max(0, min(1.0, realtime_acc))

        # 4) Decision Efficiency
        #    If real-time accuracy is good, we reduce policy lag
        #    synergy factor also helps
        decision_eff += (realtime_acc + synergy_factor * 0.01) * 0.01
        decision_eff  = max(0, min(1.0, decision_eff))

        # 5) Markov chain shock: if shock state=1 => network outage
        if shock_states[t] == 1:
            data_integration = max(data_integration - shock_impact, min_integration)
            realtime_acc     = max(realtime_acc - shock_impact*0.5, 0)
            iot_scale        = max(iot_scale - shock_impact*0.3, 0)

        # 6) Cost-Benefit
        #    Suppose each step we pay for data storage + analytics
        #    But we also gain from improved decision efficiency
        step_cost = (data_storage_cost * data_complexity) + analytics_cost * (1 - realtime_acc)
        step_savings = (decision_eff * 100.0)  # arbitrary revenue or cost saving

        net_value = step_savings - step_cost
        cumulative_savings += net_value

    # Return final time series
    return {
        "Data_Integration": data_integration_list,
        "IoT_Scalability": iot_scalability_list,
        "Data_Accuracy": data_accuracy_list,
        "Decision_Efficiency": decision_eff_list,
        "Cost_Benefit": cost_benefit_list
    }

###############################################################################
# MONTE CARLO FRAMEWORK
###############################################################################

def run_monte_carlo_simulations(n_simulations=100, time_steps=24, scenario="baseline"):
    """
    Runs multiple simulations for a chosen scenario, returns a DataFrame of final outcomes.
    """
    scenario_params = get_scenario_parameters(scenario)

    results_dict = {
        "Data_Integration": [],
        "IoT_Scalability": [],
        "Data_Accuracy": [],
        "Decision_Efficiency": [],
        "Cost_Benefit": []
    }

    for _ in range(n_simulations):
        sim_results = run_simulation(time_steps=time_steps, scenario_params=scenario_params)

        # We take the final step's value from each time series
        results_dict["Data_Integration"].append(sim_results["Data_Integration"][-1])
        results_dict["IoT_Scalability"].append(sim_results["IoT_Scalability"][-1])
        results_dict["Data_Accuracy"].append(sim_results["Data_Accuracy"][-1])
        results_dict["Decision_Efficiency"].append(sim_results["Decision_Efficiency"][-1])
        results_dict["Cost_Benefit"].append(sim_results["Cost_Benefit"][-1])

    df_final = pd.DataFrame(results_dict)
    return df_final

###############################################################################
# ANALYSIS & VISUALIZATION
###############################################################################

def analyze_uncertainty(df_results, title="Data Management Scenario Analysis"):
    """
    Analyze the distribution of final results, compute confidence intervals,
    and visualize outputs.
    """
    summary_stats = df_results.describe(percentiles=[0.025, 0.5, 0.975])
    print(f"=== {title} ===")
    print(summary_stats)

    # Example: 95% CI for Data Integration
    di_lower_95 = summary_stats.loc["2.5%", "Data_Integration"]
    di_median   = summary_stats.loc["50%",  "Data_Integration"]
    di_upper_95 = summary_stats.loc["97.5%", "Data_Integration"]

    print(f"\n95% CI for Data Integration: [{di_lower_95:.4f}, {di_upper_95:.4f}]")
    print(f"Median Data Integration: {di_median:.4f}\n")

    # Plot histograms for key outputs
    columns_to_plot = [
        "Data_Integration", "IoT_Scalability", "Data_Accuracy",
        "Decision_Efficiency", "Cost_Benefit"
    ]

    fig, axes = plt.subplots(1, 5, figsize=(20, 4))
    axes = axes.flatten()

    for i, col in enumerate(columns_to_plot):
        sns.histplot(df_results[col], ax=axes[i], kde=True)
        axes[i].set_title(col)

    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

###############################################################################
# DEMONSTRATION
###############################################################################

if __name__ == "__main__":
    # 1) Baseline scenario
    df_baseline = run_monte_carlo_simulations(n_simulations=200, time_steps=24, scenario="baseline")
    analyze_uncertainty(df_baseline, title="Baseline Scenario for Data Management")

    # 2) High Investment
    df_invest = run_monte_carlo_simulations(n_simulations=200, time_steps=24, scenario="high_investment")
    analyze_uncertainty(df_invest, title="High Investment Scenario")

    # 3) Low Resource
    df_lowres = run_monte_carlo_simulations(n_simulations=200, time_steps=24, scenario="low_resource")
    analyze_uncertainty(df_lowres, title="Low Resource Scenario")


# HOW THIS SIMULATION ADDRESSES RQ3:
# - Incorporates correlated draws for IoT adoption, AI capability, and data complexity.
# - Models random 'network outages' using a Markov chain.
# - Tracks time series of data integration, IoT scalability, data accuracy, and decision efficiency.
# - Evaluates cost-benefit from data management improvements, providing insights into
#   how leading-edge technologies (AI, IoT) can enhance urban data management in low-income contexts.