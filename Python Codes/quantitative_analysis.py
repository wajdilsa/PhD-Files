# -*- coding: utf-8 -*-
"""Quantitative Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W3GJSZLnv_QsRLW7ifNjK8TdXH-iLFFm
"""

import random

# Define the variables and their ranges
variables = {
    "Internet Penetration Rate": (10, 60),
    "Mobile Phone Penetration": (20, 70),
    "4G/5G Coverage": (5, 50),
    "E-government Development Index": (0.2, 0.6),
    "Online Service Usage": (10, 40),
    "Big Data Maturity Level": (1, 2),  # 1: Low, 2: Medium
    "Data Storage Capacity": (10, 100),  # in TB
    "AI Readiness Index": (1, 2),  # 1: Low, 2: Medium
    "AI Investment": (100000, 1000000),  # in USD
    "Renewable Energy Share": (5, 30),
    "Solar Irradiance": (3, 5),  # kWh/m2/day
    "Wind Speed": (3, 7),  # m/s
    "Energy Consumption per Capita": (500, 2000),  # kWh/person/year
    "Building Energy Efficiency": (100, 300),  # kWh/m2/year
    "Greenhouse Gas Emissions": (1000000, 5000000),  # tons of CO2 eq/year
    "Air Pollution Levels": (20, 100),  # µg/m3
    "Groundwater Level": (10, 50),  # meters below ground level
    "Water Quality Index": (0.4, 0.7),
    "Per Capita Water Consumption": (50, 150),  # liters/person/day
    "Rainwater Harvesting Capacity": (10000, 50000),  # cubic meters/year
    "Urbanization Rate": (10, 50),
    "Green Space per Capita": (1, 10),  # m2/person
    "Road Infrastructure Quality": (2, 5),  # International Roughness Index
    "Building Density": (10, 50),  # buildings/km2
    "Healthcare Access Index": (0.4, 0.7),
    "Vaccination Coverage": (50, 90),
    "Malnutrition Rate": (20, 40),  # % of children under five
    "GDP per Capita": (500, 3000),  # USD
    "Poverty Rate": (20, 50),  # % of population
    "Gini Coefficient": (0.4, 0.6),
    "Unemployment Rate": (10, 30),
    "Literacy Rate": (50, 80),  # % of population aged 15 and over
    "Female Labor Force Participation": (20, 50),  # % of women aged 15-64
    "Political Stability Index": (0.3, 0.6),
    "Corruption Perception Index": (0.2, 0.5),
    "Environmental Policy Stringency": (0.3, 0.6),
    "Public Participation Index": (0.3, 0.6),
    "Education Expenditure": (2, 5),  # % of GDP
    "Research and Development Expenditure": (0.1, 1)  # % of GDP
}

# Function to generate random values within the given ranges
def generate_initial_values(variables):
    return {var: random.uniform(*range_) if isinstance(range_, tuple) else range_ for var, range_ in variables.items()}

# Function to simulate changes after implementing the framework
def implement_framework(values):
    updated_values = values.copy()
    # Simulate improvements by increasing certain metrics by a fixed percentage or value
    improvement_factors = {
        "Internet Penetration Rate": 1.2,
        "Mobile Phone Penetration": 1.2,
        "4G/5G Coverage": 1.3,
        "E-government Development Index": 1.2,
        "Online Service Usage": 1.2,
        "Big Data Maturity Level": 1.0,  # Remain same as ranges are Low - Medium
        "Data Storage Capacity": 1.5,
        "AI Readiness Index": 1.0,  # Remain same as ranges are Low - Medium
        "AI Investment": 1.3,
        "Renewable Energy Share": 1.3,
        "Solar Irradiance": 1.0,  # Remain same as it's a natural value
        "Wind Speed": 1.0,  # Remain same as it's a natural value
        "Energy Consumption per Capita": 0.9,
        "Building Energy Efficiency": 0.8,
        "Greenhouse Gas Emissions": 0.7,
        "Air Pollution Levels": 0.7,
        "Groundwater Level": 1.0,  # Remain same as it's a natural value
        "Water Quality Index": 1.2,
        "Per Capita Water Consumption": 1.1,
        "Rainwater Harvesting Capacity": 1.4,
        "Urbanization Rate": 1.1,
        "Green Space per Capita": 1.2,
        "Road Infrastructure Quality": 1.2,
        "Building Density": 1.1,
        "Healthcare Access Index": 1.3,
        "Vaccination Coverage": 1.2,
        "Malnutrition Rate": 0.8,
        "GDP per Capita": 1.5,
        "Poverty Rate": 0.7,
        "Gini Coefficient": 0.9,
        "Unemployment Rate": 0.7,
        "Literacy Rate": 1.2,
        "Female Labor Force Participation": 1.2,
        "Political Stability Index": 1.2,
        "Corruption Perception Index": 0.8,
        "Environmental Policy Stringency": 1.3,
        "Public Participation Index": 1.3,
        "Education Expenditure": 1.2,
        "Research and Development Expenditure": 1.4
    }

    for var, factor in improvement_factors.items():
        updated_values[var] *= factor
        # Ensure values stay within their defined ranges
        min_val, max_val = variables[var]
        updated_values[var] = min(max(updated_values[var], min_val), max_val)

    return updated_values

# Generate initial values
initial_values = generate_initial_values(variables)
print("Initial Values:\n", initial_values)

# Simulate changes after implementing the framework
updated_values = implement_framework(initial_values)
print("\nUpdated Values:\n", updated_values)

import random
import matplotlib.pyplot as plt
import numpy as np

# Define the variables and their ranges
variables = {
    "Internet Penetration Rate": (10, 60),
    "Mobile Phone Penetration": (20, 70),
    "4G/5G Coverage": (5, 50),
    "E-government Development Index": (0.2, 0.6),
    "Online Service Usage": (10, 40),
    "Big Data Maturity Level": (1, 2),  # 1: Low, 2: Medium
    "Data Storage Capacity": (10, 100),  # in TB
    "AI Readiness Index": (1, 2),  # 1: Low, 2: Medium
    "AI Investment": (100000, 1000000),  # in USD
    "Renewable Energy Share": (5, 30),
    "Solar Irradiance": (3, 5),  # kWh/m2/day
    "Wind Speed": (3, 7),  # m/s
    "Energy Consumption per Capita": (500, 2000),  # kWh/person/year
    "Building Energy Efficiency": (100, 300),  # kWh/m2/year
    "Greenhouse Gas Emissions": (1000000, 5000000),  # tons of CO2 eq/year
    "Air Pollution Levels": (20, 100),  # µg/m3
    "Groundwater Level": (10, 50),  # meters below ground level
    "Water Quality Index": (0.4, 0.7),
    "Per Capita Water Consumption": (50, 150),  # liters/person/day
    "Rainwater Harvesting Capacity": (10000, 50000),  # cubic meters/year
    "Urbanization Rate": (10, 50),
    "Green Space per Capita": (1, 10),  # m2/person
    "Road Infrastructure Quality": (2, 5),  # International Roughness Index
    "Building Density": (10, 50),  # buildings/km2
    "Healthcare Access Index": (0.4, 0.7),
    "Vaccination Coverage": (50, 90),
    "Malnutrition Rate": (20, 40),  # % of children under five
    "GDP per Capita": (500, 3000),  # USD
    "Poverty Rate": (20, 50),  # % of population
    "Gini Coefficient": (0.4, 0.6),
    "Unemployment Rate": (10, 30),
    "Literacy Rate": (50, 80),  # % of population aged 15 and over
    "Female Labor Force Participation": (20, 50),  # % of women aged 15-64
    "Political Stability Index": (0.3, 0.6),
    "Corruption Perception Index": (0.2, 0.5),
    "Environmental Policy Stringency": (0.3, 0.6),
    "Public Participation Index": (0.3, 0.6),
    "Education Expenditure": (2, 5),  # % of GDP
    "Research and Development Expenditure": (0.1, 1)  # % of GDP
}

# Function to generate random values within the given ranges
def generate_initial_values(variables):
    return {var: random.uniform(*range_) if isinstance(range_, tuple) else range_ for var, range_ in variables.items()}

# Function to simulate changes after implementing the framework
def implement_framework(values):
    updated_values = values.copy()
    # Simulate improvements by increasing certain metrics by a fixed percentage or value
    improvement_factors = {
        "Internet Penetration Rate": 1.2,
        "Mobile Phone Penetration": 1.2,
        "4G/5G Coverage": 1.3,
        "E-government Development Index": 1.2,
        "Online Service Usage": 1.2,
        "Big Data Maturity Level": 1.0,  # Remain same as ranges are Low - Medium
        "Data Storage Capacity": 1.5,
        "AI Readiness Index": 1.0,  # Remain same as ranges are Low - Medium
        "AI Investment": 1.3,
        "Renewable Energy Share": 1.3,
        "Solar Irradiance": 1.0,  # Remain same as it's a natural value
        "Wind Speed": 1.0,  # Remain same as it's a natural value
        "Energy Consumption per Capita": 0.9,
        "Building Energy Efficiency": 0.8,
        "Greenhouse Gas Emissions": 0.7,
        "Air Pollution Levels": 0.7,
        "Groundwater Level": 1.0,  # Remain same as it's a natural value
        "Water Quality Index": 1.2,
        "Per Capita Water Consumption": 1.1,
        "Rainwater Harvesting Capacity": 1.4,
        "Urbanization Rate": 1.1,
        "Green Space per Capita": 1.2,
        "Road Infrastructure Quality": 1.2,
        "Building Density": 1.1,
        "Healthcare Access Index": 1.3,
        "Vaccination Coverage": 1.2,
        "Malnutrition Rate": 0.8,
        "GDP per Capita": 1.5,
        "Poverty Rate": 0.7,
        "Gini Coefficient": 0.9,
        "Unemployment Rate": 0.7,
        "Literacy Rate": 1.2,
        "Female Labor Force Participation": 1.2,
        "Political Stability Index": 1.2,
        "Corruption Perception Index": 0.8,
        "Environmental Policy Stringency": 1.3,
        "Public Participation Index": 1.3,
        "Education Expenditure": 1.2,
        "Research and Development Expenditure": 1.4
    }

    for var, factor in improvement_factors.items():
        updated_values[var] *= factor
        # Ensure values stay within their defined ranges
        min_val, max_val = variables[var]
        updated_values[var] = min(max(updated_values[var], min_val), max_val)

    return updated_values

# Generate initial values
initial_values = generate_initial_values(variables)

# Simulate changes after implementing the framework
updated_values = implement_framework(initial_values)

# Function to plot the data
def plot_values(initial_values, updated_values):
    categories = list(initial_values.keys())
    initial = list(initial_values.values())
    updated = list(updated_values.values())

    x = np.arange(len(categories))  # the label locations
    width = 0.35  # the width of the bars

    fig, ax = plt.subplots(figsize=(12, 8))
    rects1 = ax.bar(x - width/2, initial, width, label='Before Implementation')
    rects2 = ax.bar(x + width/2, updated, width, label='After Implementation')

    # Add some text for labels, title and custom x-axis tick labels, etc.
    ax.set_xlabel('Variables')
    ax.set_ylabel('Values')
    ax.set_title('City Metrics Before and After Framework Implementation')
    ax.set_xticks(x)
    ax.set_xticklabels(categories, rotation=90)
    ax.legend()

    fig.tight_layout()

    plt.show()

# Plot the initial and updated values
plot_values(initial_values, updated_values)

import random
import matplotlib.pyplot as plt
import numpy as np

# Define the variables and their ranges
variables = {
    "Internet Penetration Rate": (10, 60),
    "Mobile Phone Penetration": (20, 70),
    "4G/5G Coverage": (5, 50),
    "E-government Development Index": (0.2, 0.6),
    "Online Service Usage": (10, 40),
    "Big Data Maturity Level": (1, 2),  # 1: Low, 2: Medium
    "Data Storage Capacity": (10, 100),  # in TB
    "AI Readiness Index": (1, 2),  # 1: Low, 2: Medium
    "AI Investment": (100000, 1000000),  # in USD
    "Renewable Energy Share": (5, 30),
    "Solar Irradiance": (3, 5),  # kWh/m2/day
    "Wind Speed": (3, 7),  # m/s
    "Energy Consumption per Capita": (500, 2000),  # kWh/person/year
    "Building Energy Efficiency": (100, 300),  # kWh/m2/year
    "Greenhouse Gas Emissions": (1000000, 5000000),  # tons of CO2 eq/year
    "Air Pollution Levels": (20, 100),  # µg/m3
    "Groundwater Level": (10, 50),  # meters below ground level
    "Water Quality Index": (0.4, 0.7),
    "Per Capita Water Consumption": (50, 150),  # liters/person/day
    "Rainwater Harvesting Capacity": (10000, 50000),  # cubic meters/year
    "Urbanization Rate": (10, 50),
    "Green Space per Capita": (1, 10),  # m2/person
    "Road Infrastructure Quality": (2, 5),  # International Roughness Index
    "Building Density": (10, 50),  # buildings/km2
    "Healthcare Access Index": (0.4, 0.7),
    "Vaccination Coverage": (50, 90),
    "Malnutrition Rate": (20, 40),  # % of children under five
    "GDP per Capita": (500, 3000),  # USD
    "Poverty Rate": (20, 50),  # % of population
    "Gini Coefficient": (0.4, 0.6),
    "Unemployment Rate": (10, 30),
    "Literacy Rate": (50, 80),  # % of population aged 15 and over
    "Female Labor Force Participation": (20, 50),  # % of women aged 15-64
    "Political Stability Index": (0.3, 0.6),
    "Corruption Perception Index": (0.2, 0.5),
    "Environmental Policy Stringency": (0.3, 0.6),
    "Public Participation Index": (0.3, 0.6),
    "Education Expenditure": (2, 5),  # % of GDP
    "Research and Development Expenditure": (0.1, 1)  # % of GDP
}

# Function to generate random values within the given ranges
def generate_initial_values(variables):
    return {var: random.uniform(*range_) if isinstance(range_, tuple) else range_ for var, range_ in variables.items()}

# Function to normalize values to a 0-1 scale based on their range
def normalize_values(values, variables):
    normalized = {}
    for var, value in values.items():
        min_val, max_val = variables[var]
        normalized[var] = (value - min_val) / (max_val - min_val)
    return normalized

# Function to denormalize values from a 0-1 scale back to their original range
def denormalize_values(normalized, variables):
    denormalized = {}
    for var, value in normalized.items():
        min_val, max_val = variables[var]
        denormalized[var] = value * (max_val - min_val) + min_val
    return denormalized

# Function to simulate changes after implementing the framework with incremental improvements
def implement_framework(normalized_values, iterations=10):
    updated_values = normalized_values.copy()
    # Simulate incremental improvements over a number of iterations
    for i in range(iterations):
        for var in updated_values:
            # Apply a small improvement increment
            improvement_factor = 0.05  # 5% improvement per iteration
            updated_values[var] += improvement_factor * (1 - updated_values[var])
            updated_values[var] = min(max(updated_values[var], 0), 1)  # Ensure values stay within 0-1
    return updated_values

# Generate initial values and normalize them
initial_values = generate_initial_values(variables)
normalized_initial_values = normalize_values(initial_values, variables)

# Simulate changes after implementing the framework
normalized_updated_values = implement_framework(normalized_initial_values)
updated_values = denormalize_values(normalized_updated_values, variables)

# Function to plot the data
def plot_values(initial_values, updated_values):
    categories = list(initial_values.keys())
    initial = list(initial_values.values())
    updated = list(updated_values.values())

    x = np.arange(len(categories))  # the label locations
    width = 0.35  # the width of the bars

    fig, ax = plt.subplots(figsize=(14, 8))
    rects1 = ax.bar(x - width/2, initial, width, label='Before Implementation')
    rects2 = ax.bar(x + width/2, updated, width, label='After Implementation')

    # Add some text for labels, title and custom x-axis tick labels, etc.
    ax.set_xlabel('Variables')
    ax.set_ylabel('Values')
    ax.set_title('City Metrics Before and After Framework Implementation')
    ax.set_xticks(x)
    ax.set_xticklabels(categories, rotation=90)
    ax.legend()

    fig.tight_layout()

    plt.show()

# Plot the initial and updated values
plot_values(initial_values, updated_values)



# Import numpy as np
import numpy as np

# Function to calculate Cronbach's Alpha
def cronbach_alpha(df):
    df_corr = df.corr()
    n_items = len(df_corr)
    mean_corr = df_corr.values[np.triu_indices(n_items, k=1)].mean()
    return (n_items * mean_corr) / (1 + (n_items - 1) * mean_corr)

# Calculate Cronbach's Alpha
alpha = cronbach_alpha(df_avg)

# Display results
reliability_results = pd.DataFrame({
    "Standard Deviation": std_dev,
    "Interquartile Range (IQR)": iqr
})

reliability_results.loc["Cronbach's Alpha"] = [alpha, alpha]
import ace_tools as tools; tools.display_dataframe_to_user(name="Reliability and Consistency Test Results", dataframe=reliability_results)
reliability_results

import numpy as np
import pandas as pd

# Number of simulations
num_simulations = 1000

# Create a DataFrame to store results
results = pd.DataFrame(columns=[
    'Internet_Penetration', 'Mobile_Banking_Adoption', 'E_government_Services',
    'Big_Data_Maturity', 'AI_Applications', 'Renewable_Energy_Share',
    'Energy_Efficiency', 'Air_Pollution_Levels', 'Greenhouse_Gas_Emissions',
    'Groundwater_Levels', 'Water_Quality_Index', 'Per_Capita_Consumption',
    'Rainwater_Harvesting_Capacity', 'Urbanization_Rate', 'Green_Space_Per_Capita',
    'Public_Transport_Share', 'Private_Transport_Share', 'Building_Density',
    'Cholera_Prevalence', 'NCD_Prevalence', 'Healthcare_Access_Index',
    'Vaccination_Coverage', 'Malnutrition_Rate', 'GDP_Per_Capita', 'Poverty_Rate',
    'Gini_Coefficient', 'Unemployment_Rate', 'Literacy_Rate',
    'Female_Labor_Force_Participation', 'Political_Stability_Index',
    'Corruption_Perception_Index', 'Environmental_Policy_Stringency',
    'Public_Participation_Index', 'Education_Expenditure',
    'Research_Development_Expenditure', 'AI_Readiness_Index',
    'Machine_Learning_Adoption_Rate', 'Big_Data_Utilization', 'AI_Investment'
])

# Simulation Loop
for _ in range(num_simulations):
    # Socioeconomic Development (correlated variables)
    gdp_per_capita = np.random.lognormal(6.5, 0.8)  # Log-normal for skewed distribution
    poverty_rate = 0.8 - 0.00015 * gdp_per_capita  # Negative correlation with GDP
    gini_coefficient = 0.55 - 0.00005 * gdp_per_capita  # Negative correlation with GDP
    literacy_rate = 0.5 + 0.0001 * gdp_per_capita  # Positive correlation with GDP

    # Technology & Infrastructure (correlated with GDP)
    internet_penetration = 0.1 + 0.0002 * gdp_per_capita
    mobile_banking_adoption = 0.05 + 0.0001 * gdp_per_capita
    e_government_services = 0.2 + 0.00015 * gdp_per_capita
    big_data_maturity = 0.01 * gdp_per_capita / 1000  # Assuming a non-linear relationship
    ai_applications = 0.005 * gdp_per_capita / 1000

    # Energy & Environment (some correlated with GDP)
    renewable_energy_share = np.random.beta(2, 5)  # Beta distribution for bounded values
    energy_efficiency = 20 + 0.002 * gdp_per_capita
    air_pollution_levels = 60 - 0.005 * gdp_per_capita
    greenhouse_gas_emissions = np.random.gamma(2, 0.1)  # Gamma for right-skewed distribution

    # ... (rest of the variables with appropriate distributions and correlations)

    # Store results in the DataFrame (same as before)
    results.loc[len(results)] = [...]  # Fill in the rest of the variables

print(results)

import random
import pandas as pd

# Define variable ranges for each income group
variable_ranges = {
    "V1": {"low": (10, 40), "middle": (40, 70), "high": (70, 95)},
    "V2": {"low": (5, 25), "middle": (25, 50), "high": (50, 80)},
    "V3": {"low": (0.2, 0.4), "middle": (0.4, 0.6), "high": (0.6, 0.9)},
    "V4": {"low": (0, 20), "middle": (20, 50), "high": (50, 80)},
    "V5": {"low": (0, 10), "middle": (10, 30), "high": (30, 60)},
    "V6": {"low": (20, 50), "middle": (10, 30), "high": (20, 60)},
    "V7": {"low": (10, 30), "middle": (30, 70), "high": (70, 120)},
    "V8": {"low": (30, 70), "middle": (20, 50), "high": (5, 20)},
    "V9": {"low": (0.5, 2), "middle": (2, 6), "high": (5, 15)},
    "V10": {"low": (10, 30), "middle": (5, 20), "high": (1, 10)},
    "V11": {"low": (30, 60), "middle": (50, 75), "high": (70, 90)},
    "V12": {"low": (20, 50), "middle": (50, 100), "high": (150, 300)},
    "V13": {"low": (10, 30), "middle": (30, 60), "high": (60, 100)},
    "V14": {"low": (20, 40), "middle": (40, 70), "high": (70, 90)},
    "V15": {"low": (1, 5), "middle": (5, 20), "high": (20, 50)},
    "V16-A": {"low": (50, 80), "middle": (30, 60), "high": (20, 50)},
    "V16-B": {"low": (20, 50), "middle": (40, 70), "high": (50, 80)},
    "V17": {"low": (30, 60), "middle": (60, 120), "high": (40, 80)},
    "V18-A": {"low": (10, 100000), "middle": (1, 10), "high": (0, 1)},
    "V18-B": {"low": (30, 40), "middle": (50, 60), "high": (70, 100)},
    "V19": {"low": (30, 50), "middle": (50, 70), "high": (70, 90)},
    "V20": {"low": (50, 70), "middle": (70, 90), "high": (90, 95)},
    "V21": {"low": (20, 100), "middle": (10, 20), "high": (0, 10)},
    "V22": {"low": (1000, 4000), "middle": (4000, 12000), "high": (12000, 50000)},
    "V23": {"low": (40, 70), "middle": (10, 40), "high": (0, 10)},
    "V24": {"low": (0.45, 0.6), "middle": (0.3, 0.45), "high": (0.25, 0.35)},
    "V25": {"low": (5, 15), "middle": (3, 10), "high": (3, 7)},
    "V26": {"low": (50, 75), "middle": (75, 90), "high": (90, 100)},
    "V27": {"low": (40, 60), "middle": (50, 70), "high": (60, 80)},
    "V28": {"low": (-2.5, -1.0), "middle": (-1.0, 0.5), "high": (0.5, 2.5)},
    "V29": {"low": (10, 30), "middle": (31, 60), "high": (61, 90)},
    "V30": {"low": (0, 2), "middle": (2, 4), "high": (4, 6)},
    "V31": {"low": (0, 2), "middle": (2, 4), "high": (4, 6)},
    "V32": {"low": (40, 70), "middle": (70, 90), "high": (90, 100)},
    "V33": {"low": (2, 4), "middle": (3, 6), "high": (5, 7)},
    "V34": {"low": (0.1, 0.5), "middle": (0.5, 1.5), "high": (2, 4)},
    "V35": {"low": (20, 40), "middle": (40, 60), "high": (60, 90)},
    "V36": {"low": (5, 10), "middle": (10, 25), "high": (25, 50)},
    "V37": {"low": (5, 15), "middle": (15, 35), "high": (35, 60)},
    "V38": {"low": (0.01, 0.05), "middle": (0.05, 0.1), "high": (0.1, 0.5)}
}

# Generate random data for each income group
def generate_data(income_group, num_countries=10):
    data = []
    for _ in range(num_countries):
        country_data = {"Income Group": income_group}
        for var, ranges in variable_ranges.items():
            country_data[var] = random.uniform(*ranges[income_group])
        data.append(country_data)
    return data

# Generate data for each income group
low_income_data = generate_data("low")
middle_income_data = generate_data("middle")
high_income_data = generate_data("high")

# Combine all data
all_data = low_income_data + middle_income_data + high_income_data

# Create DataFrame
df = pd.DataFrame(all_data)

import ace_tools as tools; tools.display_dataframe_to_user(name="Simulated Country Data", dataframe=df)

# Display the DataFrame
df.head()

import random
import pandas as pd

# Define variable ranges for each income group
variable_ranges = {
    "V1": {"low": (10, 40), "middle": (40, 70), "high": (70, 95)},
    "V2": {"low": (5, 25), "middle": (25, 50), "high": (50, 80)},
    "V3": {"low": (0.2, 0.4), "middle": (0.4, 0.6), "high": (0.6, 0.9)},
    "V4": {"low": (0, 20), "middle": (20, 50), "high": (50, 80)},
    "V5": {"low": (0, 10), "middle": (10, 30), "high": (30, 60)},
    "V6": {"low": (20, 50), "middle": (10, 30), "high": (20, 60)},
    "V7": {"low": (10, 30), "middle": (30, 70), "high": (70, 120)},
    "V8": {"low": (30, 70), "middle": (20, 50), "high": (5, 20)},
    "V9": {"low": (0.5, 2), "middle": (2, 6), "high": (5, 15)},
    "V10": {"low": (10, 30), "middle": (5, 20), "high": (1, 10)},
    "V11": {"low": (30, 60), "middle": (50, 75), "high": (70, 90)},
    "V12": {"low": (20, 50), "middle": (50, 100), "high": (150, 300)},
    "V13": {"low": (10, 30), "middle": (30, 60), "high": (60, 100)},
    "V14": {"low": (20, 40), "middle": (40, 70), "high": (70, 90)},
    "V15": {"low": (1, 5), "middle": (5, 20), "high": (20, 50)},
    "V16-A": {"low": (50, 80), "middle": (30, 60), "high": (20, 50)},
    "V16-B": {"low": (20, 50), "middle": (40, 70), "high": (50, 80)},
    "V17": {"low": (30, 60), "middle": (60, 120), "high": (40, 80)},
    "V18-A": {"low": (10, 100000), "middle": (1, 10), "high": (0, 1)},
    "V18-B": {"low": (30, 40), "middle": (50, 60), "high": (70, 100)},
    "V19": {"low": (30, 50), "middle": (50, 70), "high": (70, 90)},
    "V20": {"low": (50, 70), "middle": (70, 90), "high": (90, 95)},
    "V21": {"low": (20, 100), "middle": (10, 20), "high": (0, 10)},
    "V22": {"low": (1000, 4000), "middle": (4000, 12000), "high": (12000, 50000)},
    "V23": {"low": (40, 70), "middle": (10, 40), "high": (0, 10)},
    "V24": {"low": (0.45, 0.6), "middle": (0.3, 0.45), "high": (0.25, 0.35)},
    "V25": {"low": (5, 15), "middle": (3, 10), "high": (3, 7)},
    "V26": {"low": (50, 75), "middle": (75, 90), "high": (90, 100)},
    "V27": {"low": (40, 60), "middle": (50, 70), "high": (60, 80)},
    "V28": {"low": (-2.5, -1.0), "middle": (-1.0, 0.5), "high": (0.5, 2.5)},
    "V29": {"low": (10, 30), "middle": (31, 60), "high": (61, 90)},
    "V30": {"low": (0, 2), "middle": (2, 4), "high": (4, 6)},
    "V31": {"low": (0, 2), "middle": (2, 4), "high": (4, 6)},
    "V32": {"low": (40, 70), "middle": (70, 90), "high": (90, 100)},
    "V33": {"low": (2, 4), "middle": (3, 6), "high": (5, 7)},
    "V34": {"low": (0.1, 0.5), "middle": (0.5, 1.5), "high": (2, 4)},
    "V35": {"low": (20, 40), "middle": (40, 60), "high": (60, 90)},
    "V36": {"low": (5, 10), "middle": (10, 25), "high": (25, 50)},
    "V37": {"low": (5, 15), "middle": (15, 35), "high": (35, 60)},
    "V38": {"low": (0.01, 0.05), "middle": (0.05, 0.1), "high": (0.1, 0.5)}
}

# Generate random data for each income group
def generate_data(income_group, num_countries=10):
    data = []
    for _ in range(num_countries):
        country_data = {"Income Group": income_group}
        for var, ranges in variable_ranges.items():
            country_data[var] = random.uniform(*ranges[income_group])
        data.append(country_data)
    return data

# Generate data for each income group
low_income_data = generate_data("low")
middle_income_data = generate_data("middle")
high_income_data = generate_data("high")

# Combine all data
all_data = low_income_data + middle_income_data + high_income_data

# Create DataFrame
df = pd.DataFrame(all_data)

import ace_tools as tools; tools.display_dataframe_to_user(name="Simulated Country Data", dataframe=df)

# Display the DataFrame
df.head()

!pip install ace

import random
import pandas as pd

# Define variable ranges for each income group
variable_ranges = {
    "V1": {"low": (10, 40), "middle": (40, 70), "high": (70, 95)},
    "V2": {"low": (5, 25), "middle": (25, 50), "high": (50, 80)},
    "V3": {"low": (0.2, 0.4), "middle": (0.4, 0.6), "high": (0.6, 0.9)},
    "V4": {"low": (0, 20), "middle": (20, 50), "high": (50, 80)},
    "V5": {"low": (0, 10), "middle": (10, 30), "high": (30, 60)},
    "V6": {"low": (20, 50), "middle": (10, 30), "high": (20, 60)},
    "V7": {"low": (10, 30), "middle": (30, 70), "high": (70, 120)},
    "V8": {"low": (30, 70), "middle": (20, 50), "high": (5, 20)},
    "V9": {"low": (0.5, 2), "middle": (2, 6), "high": (5, 15)},
    "V10": {"low": (10, 30), "middle": (5, 20), "high": (1, 10)},
    "V11": {"low": (30, 60), "middle": (50, 75), "high": (70, 90)},
    "V12": {"low": (20, 50), "middle": (50, 100), "high": (150, 300)},
    "V13": {"low": (10, 30), "middle": (30, 60), "high": (60, 100)},
    "V14": {"low": (20, 40), "middle": (40, 70), "high": (70, 90)},
    "V15": {"low": (1, 5), "middle": (5, 20), "high": (20, 50)},
    "V16-A": {"low": (50, 80), "middle": (30, 60), "high": (20, 50)},
    "V16-B": {"low": (20, 50), "middle": (40, 70), "high": (50, 80)},
    "V17": {"low": (30, 60), "middle": (60, 120), "high": (40, 80)},
    "V18-A": {"low": (10, 100000), "middle": (1, 10), "high": (0, 1)},
    "V18-B": {"low": (30, 40), "middle": (50, 60), "high": (70, 100)},
    "V19": {"low": (30, 50), "middle": (50, 70), "high": (70, 90)},
    "V20": {"low": (50, 70), "middle": (70, 90), "high": (90, 95)},
    "V21": {"low": (20, 100), "middle": (10, 20), "high": (0, 10)},
    "V22": {"low": (1000, 4000), "middle": (4000, 12000), "high": (12000, 50000)},
    "V23": {"low": (40, 70), "middle": (10, 40), "high": (0, 10)},
    "V24": {"low": (0.45, 0.6), "middle": (0.3, 0.45), "high": (0.25, 0.35)},
    "V25": {"low": (5, 15), "middle": (3, 10), "high": (3, 7)},
    "V26": {"low": (50, 75), "middle": (75, 90), "high": (90, 100)},
    "V27": {"low": (40, 60), "middle": (50, 70), "high": (60, 80)},
    "V28": {"low": (-2.5, -1.0), "middle": (-1.0, 0.5), "high": (0.5, 2.5)},
    "V29": {"low": (10, 30), "middle": (31, 60), "high": (61, 90)},
    "V30": {"low": (0, 2), "middle": (2, 4), "high": (4, 6)},
    "V31": {"low": (0, 2), "middle": (2, 4), "high": (4, 6)},
    "V32": {"low": (40, 70), "middle": (70, 90), "high": (90, 100)},
    "V33": {"low": (2, 4), "middle": (3, 6), "high": (5, 7)},
    "V34": {"low": (0.1, 0.5), "middle": (0.5, 1.5), "high": (2, 4)},
    "V35": {"low": (20, 40), "middle": (40, 60), "high": (60, 90)},
    "V36": {"low": (5, 10), "middle": (10, 25), "high": (25, 50)},
    "V37": {"low": (5, 15), "middle": (15, 35), "high": (35, 60)},
    "V38": {"low": (0.01, 0.05), "middle": (0.05, 0.1), "high": (0.1, 0.5)}
}

# Generate random data for each income group
def generate_data(income_group, num_countries=10):
    data = []
    for _ in range(num_countries):
        country_data = {"Income Group": income_group}
        for var, ranges in variable_ranges.items():
            country_data[var] = random.uniform(*ranges[income_group])
        data.append(country_data)
    return data

# Generate data for each income group
low_income_data = generate_data("low")
middle_income_data = generate_data("middle")
high_income_data = generate_data("high")

# Combine all data
all_data = low_income_data + middle_income_data + high_income_data

# Create DataFrame
df = pd.DataFrame(all_data)

# Display the DataFrame
print(df.head())

import random
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Define variable ranges for each income group
variable_ranges = {
    "V1": {"low": (10, 40), "middle": (40, 70), "high": (70, 95)},
    "V2": {"low": (5, 25), "middle": (25, 50), "high": (50, 80)},
    "V3": {"low": (0.2, 0.4), "middle": (0.4, 0.6), "high": (0.6, 0.9)},
    "V4": {"low": (0, 20), "middle": (20, 50), "high": (50, 80)},
    "V5": {"low": (0, 10), "middle": (10, 30), "high": (30, 60)},
    "V6": {"low": (20, 50), "middle": (10, 30), "high": (20, 60)},
    "V7": {"low": (10, 30), "middle": (30, 70), "high": (70, 120)},
    "V8": {"low": (30, 70), "middle": (20, 50), "high": (5, 20)},
    "V9": {"low": (0.5, 2), "middle": (2, 6), "high": (5, 15)},
    "V10": {"low": (10, 30), "middle": (5, 20), "high": (1, 10)},
    "V11": {"low": (30, 60), "middle": (50, 75), "high": (70, 90)},
    "V12": {"low": (20, 50), "middle": (50, 100), "high": (150, 300)},
    "V13": {"low": (10, 30), "middle": (30, 60), "high": (60, 100)},
    "V14": {"low": (20, 40), "middle": (40, 70), "high": (70, 90)},
    "V15": {"low": (1, 5), "middle": (5, 20), "high": (20, 50)},
    "V16-A": {"low": (50, 80), "middle": (30, 60), "high": (20, 50)},
    "V16-B": {"low": (20, 50), "middle": (40, 70), "high": (50, 80)},
    "V17": {"low": (30, 60), "middle": (60, 120), "high": (40, 80)},
    "V18-A": {"low": (10, 100000), "middle": (1, 10), "high": (0, 1)},
    "V18-B": {"low": (30, 40), "middle": (50, 60), "high": (70, 100)},
    "V19": {"low": (30, 50), "middle": (50, 70), "high": (70, 90)},
    "V20": {"low": (50, 70), "middle": (70, 90), "high": (90, 95)},
    "V21": {"low": (20, 100), "middle": (10, 20), "high": (0, 10)},
    "V22": {"low": (1000, 4000), "middle": (4000, 12000), "high": (12000, 50000)},
    "V23": {"low": (40, 70), "middle": (10, 40), "high": (0, 10)},
    "V24": {"low": (0.45, 0.6), "middle": (0.3, 0.45), "high": (0.25, 0.35)},
    "V25": {"low": (5, 15), "middle": (3, 10), "high": (3, 7)},
    "V26": {"low": (50, 75), "middle": (75, 90), "high": (90, 100)},
    "V27": {"low": (40, 60), "middle": (50, 70), "high": (60, 80)},
    "V28": {"low": (-2.5, -1.0), "middle": (-1.0, 0.5), "high": (0.5, 2.5)},
    "V29": {"low": (10, 30), "middle": (31, 60), "high": (61, 90)},
    "V30": {"low": (0, 2), "middle": (2, 4), "high": (4, 6)},
    "V31": {"low": (0, 2), "middle": (2, 4), "high": (4, 6)},
    "V32": {"low": (40, 70), "middle": (70, 90), "high": (90, 100)},
    "V33": {"low": (2, 4), "middle": (3, 6), "high": (5, 7)},
    "V34": {"low": (0.1, 0.5), "middle": (0.5, 1.5), "high": (2, 4)},
    "V35": {"low": (20, 40), "middle": (40, 60), "high": (60, 90)},
    "V36": {"low": (5, 10), "middle": (10, 25), "high": (25, 50)},
    "V37": {"low": (5, 15), "middle": (15, 35), "high": (35, 60)},
    "V38": {"low": (0.01, 0.05), "middle": (0.05, 0.1), "high": (0.1, 0.5)}
}

# Generate random data for each income group
def generate_data(income_group, num_countries=10):
    data = []
    for _ in range(num_countries):
        country_data = {"Income Group": income_group}
        for var, ranges in variable_ranges.items():
            country_data[var] = random.uniform(*ranges[income_group])
        data.append(country_data)
    return data

# Generate data for each income group
low_income_data = generate_data("low")
middle_income_data = generate_data("middle")
high_income_data = generate_data("high")

# Combine all data
all_data = low_income_data + middle_income_data + high_income_data

# Create DataFrame
df = pd.DataFrame(all_data)

# Display the first few rows of the DataFrame
print(df.head())

# EDA - Visualizations
plt.figure(figsize=(12, 8))
sns.boxplot(x='Income Group', y='V1', data=df)
plt.title('Internet Penetration by Income Group')
plt.show()

plt.figure(figsize=(12, 8))
sns.boxplot(x='Income Group', y='V2', data=df)
plt.title('Mobile Banking Adoption by Income Group')
plt.show()

plt.figure(figsize=(12, 8))
sns.boxplot(x='Income Group', y='V6', data=df)
plt.title('Renewable Energy Share by Income Group')
plt.show()

plt.figure(figsize=(12, 8))
sns.boxplot(x='Income Group', y='V22', data=df)
plt.title('GDP per Capita by Income Group')
plt.show()

plt.figure(figsize=(12, 8))
sns.boxplot(x='Income Group', y='V23', data=df)
plt.title('Poverty Rate by Income Group')
plt.show()

plt.figure(figsize=(12, 8))
sns.boxplot(x='Income Group', y='V27', data=df)
plt.title('Female Labor Force Participation by Income Group')
plt.show()

import random
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Define variable ranges for each income group
variable_ranges = {
    "V1": {"low": (10, 40), "middle": (40, 70), "high": (70, 95)},
    "V2": {"low": (5, 25), "middle": (25, 50), "high": (50, 80)},
    "V3": {"low": (0.2, 0.4), "middle": (0.4, 0.6), "high": (0.6, 0.9)},
    "V4": {"low": (0, 20), "middle": (20, 50), "high": (50, 80)},
    "V5": {"low": (0, 10), "middle": (10, 30), "high": (30, 60)},
    "V6": {"low": (20, 50), "middle": (10, 30), "high": (20, 60)},
    "V7": {"low": (10, 30), "middle": (30, 70), "high": (70, 120)},
    "V8": {"low": (30, 70), "middle": (20, 50), "high": (5, 20)},
    "V9": {"low": (0.5, 2), "middle": (2, 6), "high": (5, 15)},
    "V10": {"low": (10, 30), "middle": (5, 20), "high": (1, 10)},
    "V11": {"low": (30, 60), "middle": (50, 75), "high": (70, 90)},
    "V12": {"low": (20, 50), "middle": (50, 100), "high": (150, 300)},
    "V13": {"low": (10, 30), "middle": (30, 60), "high": (60, 100)},
    "V14": {"low": (20, 40), "middle": (40, 70), "high": (70, 90)},
    "V15": {"low": (1, 5), "middle": (5, 20), "high": (20, 50)},
    "V16-A": {"low": (50, 80), "middle": (30, 60), "high": (20, 50)},
    "V16-B": {"low": (20, 50), "middle": (40, 70), "high": (50, 80)},
    "V17": {"low": (30, 60), "middle": (60, 120), "high": (40, 80)},
    "V18-A": {"low": (10, 100000), "middle": (1, 10), "high": (0, 1)},
    "V18-B": {"low": (30, 40), "middle": (50, 60), "high": (70, 100)},
    "V19": {"low": (30, 50), "middle": (50, 70), "high": (70, 90)},
    "V20": {"low": (50, 70), "middle": (70, 90), "high": (90, 95)},
    "V21": {"low": (20, 100), "middle": (10, 20), "high": (0, 10)},
    "V22": {"low": (1000, 4000), "middle": (4000, 12000), "high": (12000, 50000)},
    "V23": {"low": (40, 70), "middle": (10, 40), "high": (0, 10)},
    "V24": {"low": (0.45, 0.6), "middle": (0.3, 0.45), "high": (0.25, 0.35)},
    "V25": {"low": (5, 15), "middle": (3, 10), "high": (3, 7)},
    "V26": {"low": (50, 75), "middle": (75, 90), "high": (90, 100)},
    "V27": {"low": (40, 60), "middle": (50, 70), "high": (60, 80)},
    "V28": {"low": (-2.5, -1.0), "middle": (-1.0, 0.5), "high": (0.5, 2.5)},
    "V29": {"low": (10, 30), "middle": (31, 60), "high": (61, 90)},
    "V30": {"low": (0, 2), "middle": (2, 4), "high": (4, 6)},
    "V31": {"low": (0, 2), "middle": (2, 4), "high": (4, 6)},
    "V32": {"low": (40, 70), "middle": (70, 90), "high": (90, 100)},
    "V33": {"low": (2, 4), "middle": (3, 6), "high": (5, 7)},
    "V34": {"low": (0.1, 0.5), "middle": (0.5, 1.5), "high": (2, 4)},
    "V35": {"low": (20, 40), "middle": (40, 60), "high": (60, 90)},
    "V36": {"low": (5, 10), "middle": (10, 25), "high": (25, 50)},
    "V37": {"low": (5, 15), "middle": (15, 35), "high": (35, 60)},
    "V38": {"low": (0.01, 0.05), "middle": (0.05, 0.1), "high": (0.1, 0.5)}
}

# Generate random data for each income group
def generate_data(income_group, num_countries=10):
    data = []
    for _ in range(num_countries):
        country_data = {"Income Group": income_group}
        for var, ranges in variable_ranges.items():
            country_data[var] = random.uniform(*ranges[income_group])
        data.append(country_data)
    return data

# Generate data for each income group
low_income_data = generate_data("low")
middle_income_data = generate_data("middle")
high_income_data = generate_data("high")

# Combine all data
all_data = low_income_data + middle_income_data + high_income_data

# Create DataFrame
df = pd.DataFrame(all_data)

# Display the first few rows of the DataFrame
print(df.head())

# EDA - Summary Statistics
summary_stats = df.describe()
print("Summary Statistics:")
print(summary_stats)

# EDA - Visualizations
def plot_variable_distribution(var):
    plt.figure(figsize=(12, 8))
    sns.histplot(df[var], kde=True, bins=20)
    plt.title(f'Distribution of {var}')
    plt.xlabel(var)
    plt.ylabel('Frequency')
    plt.show()

def plot_boxplot_by_income_group(var):
    plt.figure(figsize=(12, 8))
    sns.boxplot(x='Income Group', y=var, data=df)
    plt.title(f'{var} by Income Group')
    plt.show()

variables_to_analyze = ['V1', 'V2', 'V6', 'V22', 'V23', 'V27']

for var in variables_to_analyze:
    plot_variable_distribution(var)
    plot_boxplot_by_income_group(var)

# Pairplot to see relationships between variables
sns.pairplot(df, hue='Income Group', vars=variables_to_analyze)
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define the number of simulations
num_simulations = 10000

# Define input variable distributions
iot_sensor_cost = np.random.uniform(50, 200, num_simulations)  # Cost per IoT sensor
data_point_cost = np.random.normal(0.50, 0.10, num_simulations)  # Cost per data point
resource_efficiency_gain = np.random.triangular(10, 25, 50, num_simulations)  # Efficiency gain (%)
adaptive_capacity = np.random.uniform(20, 80, num_simulations)  # Adaptive capacity improvement (%)

# Define relationships
total_cost = iot_sensor_cost + data_point_cost
cost_effectiveness = resource_efficiency_gain / total_cost
urban_resilience = adaptive_capacity * resource_efficiency_gain

# Create a DataFrame to store results
results = pd.DataFrame({
    'IoT Sensor Cost': iot_sensor_cost,
    'Data Point Cost': data_point_cost,
    'Resource Efficiency Gain (%)': resource_efficiency_gain,
    'Adaptive Capacity (%)': adaptive_capacity,
    'Total Cost': total_cost,
    'Cost Effectiveness': cost_effectiveness,
    'Urban Resilience': urban_resilience
})

# Analyze Results
summary = results.describe()

# Visualize Results
plt.hist(results['Cost Effectiveness'], bins=50, alpha=0.7, label='Cost Effectiveness')
plt.hist(results['Urban Resilience'], bins=50, alpha=0.7, label='Urban Resilience')
plt.legend()
plt.title('Monte Carlo Simulation Results')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.show()

import numpy as np
import pandas as pd

# Define the number of simulations
num_simulations = 10000

# Generate random values for key variables
sensor_cost = np.random.uniform(50, 200, num_simulations)  # IoT sensor cost
efficiency_gain = np.random.triangular(10, 25, 50, num_simulations)  # Efficiency improvement (%)
data_collection_adoption = np.random.normal(55, 10, num_simulations)  # Mobile-based data collection adoption (%)
resilience_score = np.random.uniform(20, 80, num_simulations)  # Urban resilience score (%)
collaboration_index = np.random.poisson(5, num_simulations)  # Average community collaborations

# Create a DataFrame for the simulation results
simulation_data = pd.DataFrame({
    'IoT Sensor Cost': sensor_cost,
    'Efficiency Gain (%)': efficiency_gain,
    'Data Collection Adoption (%)': data_collection_adoption,
    'Resilience Score (%)': resilience_score,
    'Collaboration Index': collaboration_index
})

# Display summary statistics
simulation_summary = simulation_data.describe()

# Display the results to the user
#import ace_tools as tools; tools.display_dataframe_to_user(name="Low-Income Country Simulation Results", dataframe=simulation_data)

import pandas as pd
import numpy as np

# Define variable ranges and categories
variables = {
    "Average cost per IoT sensor unit deployed ($)": (10, 50),
    "Number of datasets managed using open-source platforms": (1, 10),
    "Percentage of total data collected via mobile applications (%)": (20, 70),
    "Number of data points contributed by community members": (500, 10000),
    "Number of active collaborations or data-sharing agreements": (2, 5),
    "Number of satellite datasets integrated into urban systems": (1, 5),
    "Reduction in local hardware costs due to cloud-based AI adoption (%)": (20, 50),
    "Percentage reduction in required training data due to transfer learning (%)": (30, 60),
    "Number of AI tools/frameworks downloaded or adopted": (5, 15),
    "Number of AI projects developed in collaboration with other cities": (1, 3),
    "Accuracy of NLP models in local language text processing (%)": (70, 85),
    "Number of participants completing AI training programs": (10, 50),
    "Number of user interactions with climate education chatbots": (1000, 10000),
    "Number of app users completing sustainability challenges": (100, 1000),
    "Number of community-led action plans submitted through the tool": (5, 20),
    "Number of queries handled by voice-based climate services": (1000, 5000),
    "Percentage of funds allocated based on participatory input (%)": (10, 30),
    "Number of schools implementing the curriculum": (10, 50),
    "Average internet speed in the target area (Mbps)": (1, 10),
    "Average latency in transmitting data across networks (ms)": (50, 200),
    "Number of processes executed via cloud platforms (per month)": (100, 500),
}

# Generate a synthetic dataset (100 samples)
num_samples = 300
synthetic_data = {
    variable: np.random.uniform(low=low, high=high, size=num_samples)
    for variable, (low, high) in variables.items()
}

# Create a DataFrame
df_synthetic = pd.DataFrame(synthetic_data)

# Save the synthetic dataset to a CSV file
csv_file_path = "Synthetic_Dataset_for_PCA.csv"
df_synthetic.to_csv(csv_file_path, index=False)

# Provide the path to the user
csv_file_path


# Display the synthetic dataset to the user
#import ace_tools as tools; tools.display_dataframe_to_user(name="Synthetic Dataset for PCA", dataframe=df_synthetic)



import pandas as pd
from factor_analyzer.factor_analyzer import calculate_kmo

# Example: Synthetic data (replace with your dataset)
data = df_synthetic

# Compute KMO measure
kmo_all, kmo_model = calculate_kmo(data)

# Output results
print(f"KMO for each variable: {kmo_all}")
print(f"Overall KMO: {kmo_model}")



! pip install factor_analyzer

df_synthetic

import pandas as pd
from factor_analyzer.factor_analyzer import calculate_kmo

# Load the dataset
df = pd.read_csv("Domain_Knowledge_Synthetic_Dataset.csv")

# Calculate KMO measure
kmo_all, kmo_model = calculate_kmo(df)

# Output KMO results
print("KMO for each variable:", kmo_all)
print("Overall KMO:", kmo_model)

import numpy as np
import pandas as pd

# Function to generate correlated data
def generate_correlated_data(mean1, mean2, std1, std2, correlation, size):
    cov_matrix = [[std1**2, correlation * std1 * std2],
                  [correlation * std1 * std2, std2**2]]
    data = np.random.multivariate_normal([mean1, mean2], cov_matrix, size=size)
    return data[:, 0], data[:, 1]

# Variables and their configurations
variables = {
    "VAR001_VAR002": {"mean1": 100, "std1": 10, "mean2": 500, "std2": 50, "correlation": -0.8},
    "VAR006_VAR009": {"mean1": 200, "std1": 30, "mean2": 1000, "std2": 100, "correlation": 0.7},
    "VAR007_VAR021": {"mean1": 50, "std1": 5, "mean2": 300, "std2": 30, "correlation": 0.6},
    "VAR025_VAR012": {"mean1": 20, "std1": 3, "mean2": 100, "std2": 10, "correlation": 0.8},
    "VAR015_VAR026": {"mean1": 50, "std1": 7, "mean2": 200, "std2": 20, "correlation": 0.75},
    "VAR019_VAR020": {"mean1": 100, "std1": 10, "mean2": 50, "std2": 5, "correlation": -0.9},
    "VAR010_VAR005": {"mean1": 30, "std1": 5, "mean2": 15, "std2": 2, "correlation": 0.85},
    "VAR011_VAR013": {"mean1": 0.9, "std1": 0.05, "mean2": 500, "std2": 50, "correlation": 0.65},
}

# Generate data
size = 100  # Number of samples
data = {}

for var_pair, config in variables.items():
    var1, var2 = generate_correlated_data(
        config["mean1"], config["mean2"], config["std1"], config["std2"], config["correlation"], size
    )
    data[f"{var_pair.split('_')[0]}"] = var1
    data[f"{var_pair.split('_')[1]}"] = var2

# Convert to DataFrame
df = pd.DataFrame(data)

# Display DataFrame to user
import ace_tools as tools; tools.display_dataframe_to_user(name="Correlated Variable Data", dataframe=df)

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from scipy.stats import pearsonr

# Function to generate correlated data
def generate_correlated_data(mean1, mean2, std1, std2, correlation, size):
    cov_matrix = [[std1**2, correlation * std1 * std2],
                  [correlation * std1 * std2, std2**2]]
    data = np.random.multivariate_normal([mean1, mean2], cov_matrix, size=size)
    return data[:, 0], data[:, 1]

# Function to calculate the Kaiser-Meyer-Olkin (KMO) measure
def calculate_kmo(df):
    corr_matrix = df.corr()
    inv_corr_matrix = np.linalg.inv(corr_matrix)
    partial_corr_matrix = -inv_corr_matrix / np.sqrt(np.outer(np.diag(inv_corr_matrix), np.diag(inv_corr_matrix)))
    np.fill_diagonal(partial_corr_matrix, 0)
    partial_corr_matrix_squared = partial_corr_matrix ** 2
    corr_matrix_squared = corr_matrix ** 2
    np.fill_diagonal(corr_matrix_squared.values, 0)
    kmo_num = corr_matrix_squared.sum().sum()
    kmo_denom = kmo_num + partial_corr_matrix_squared.sum().sum()
    kmo = kmo_num / kmo_denom
    return kmo

# Variables and their configurations
variables = {
    "VAR001_VAR002": {"mean1": 100, "std1": 10, "mean2": 500, "std2": 50, "correlation": -0.8},
    "VAR006_VAR009": {"mean1": 200, "std1": 30, "mean2": 1000, "std2": 100, "correlation": 0.7},
    "VAR007_VAR021": {"mean1": 50, "std1": 5, "mean2": 300, "std2": 30, "correlation": 0.6},
    "VAR025_VAR012": {"mean1": 20, "std1": 3, "mean2": 100, "std2": 10, "correlation": 0.8},
    "VAR015_VAR026": {"mean1": 50, "std1": 7, "mean2": 200, "std2": 20, "correlation": 0.75},
    "VAR019_VAR020": {"mean1": 100, "std1": 10, "mean2": 50, "std2": 5, "correlation": -0.9},
    "VAR010_VAR005": {"mean1": 30, "std1": 5, "mean2": 15, "std2": 2, "correlation": 0.85},
    "VAR011_VAR013": {"mean1": 0.9, "std1": 0.05, "mean2": 500, "std2": 50, "correlation": 0.65},
}

# Generate data
size = 1000  # Number of samples
data = {}

for var_pair, config in variables.items():
    var1, var2 = generate_correlated_data(
        config["mean1"], config["mean2"], config["std1"], config["std2"], config["correlation"], size
    )
    data[f"{var_pair.split('_')[0]}"] = var1
    data[f"{var_pair.split('_')[1]}"] = var2

# Convert to DataFrame
df = pd.DataFrame(data)

# Calculate KMO
kmo_value = calculate_kmo(df)

# Display DataFrame to user
#import ace_tools as tools; tools.display_dataframe_to_user(name="Correlated Variable Data", dataframe=df)

# Display KMO value
kmo_value