# -*- coding: utf-8 -*-
"""Data Generator for all Countries V01 KMO Method.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zmLc-WmYjDOqOz2nF7oqn9dhIZPiv55e
"""

! pip install factor_analyzer

"""**Data Synth. for Medium  Priority**"""

import numpy as np
import pandas as pd
from sklearn.datasets import make_spd_matrix
from factor_analyzer.factor_analyzer import calculate_kmo, calculate_bartlett_sphericity

# Variables as provided
variables = [
    "Number of datasets managed using open-source platforms",
    "Number of data points contributed by community members",
    "Number of active collaborations or data-sharing agreements",
    "Number of satellite datasets integrated into urban systems",
    "Number of AI tools/frameworks downloaded or adopted",
    "Number of AI projects developed in collaboration with other cities",
    "Number of participants completing AI training programs",
    "Number of user interactions with climate education chatbots",
    "Number of app users completing sustainability challenges",
    "Number of community-led action plans submitted through the tool",
    "Number of queries handled by voice-based climate services",
    "Number of schools implementing the curriculum",
    "Number of processes executed via cloud platforms",
    "Number of computational nodes available for urban applications",
    "Number of software licenses/tools actively in use",
    "Number of training sessions conducted annually",
    "Number of participants in community engagement activities",
    "Number of shared trips coordinated through AI platforms",
    "Number of circular economy initiatives identified and implemented",
    "Number of zoning changes implemented based on AI recommendations",
    "Number of upgraded informal settlements based on AI strategies",
    "Number of water quality parameters monitored in real-time",
    "Number of air quality monitoring stations established",
    "Number of housing units retrofitted based on AI prioritization",
    "Number of buildings audited using smartphone technology",
    "Number of IoT devices deployed",
    "Number of GIS layers used in urban planning",
    "Number of risks identified and mitigated using AI",
    "Number of community members trained through AI simulations",
    "Number of threats detected and mitigated in real time",
    "Number of security breaches prevented",
    "Number of patients treated via AI-enabled telemedicine",
    "Number of tutoring sessions conducted via AI systems",
    "Number of educational materials created and delivered using AI",
    "Number of individuals trained in AI and emerging technologies",
    "Number of critical structures monitored using low-cost sensors",
    "Number of participants trained in disaster preparedness",
    "Number of vulnerabilities identified through mapping efforts",
    "Number of disease outbreaks detected through surveillance systems",
    "Number of individuals employed in green jobs post-training",
    "Number of microenterprises established for climate-related solutions",
    "Number of successful job placements through AI-assisted matching",
    "Number of community gardens established in optimized locations",
    "Number of decisions with documented transparency and fairness",
    "Number of collaborative projects initiated through the platform",
    "Number of ethical guidelines integrated into AI projects",
    "Number of insights generated from analyzed literature",
    "Number of microfinance loans approved for green projects",
    "Number of green bond projects with documented impact reports",
    "Number of kits distributed and utilized by local innovators",
    "Number of successful matches for innovation challenges",
    "Number of startups supported through virtual incubation",
    "Number of innovations documented in the repository",
    "Number of successful skill-sharing interactions on the platform",
    "Number of projects ranked using the prioritisation matrix",
    "Number of additional locations where the solution has been implemented",
    "Number of officials trained in AI and climate-related skills",
    "Number of workshops conducted annually",
    "Number of collaborative projects with educational institutions",
    "Number of decisions with publicly accessible AI decision logs",
    "Number of data-sharing agreements adhering to ownership policies",
    "Number of active communication channels maintained",
    "Number of skilled personnel trained and retained",
    "Number of impact assessments completed using AI tools",
    "Number of real-time dashboards actively monitored",
    "Number of actionable suggestions collected from community feedback",
    "Number of environmental parameters monitored",
    "Number of urban metrics tracked (air quality)",
    "Number of visualizations created for decision-making processes",
    "Number of adaptive management strategies successfully implemented",
    "Number of scenarios analyzed to inform decision-making",
    "Number of variables assessed for sensitivity impact on outcomes",
    "Number of policy scenarios evaluated for effectiveness",
    "Number of alternative resource plans developed",
    "Number of social indicators improved through interventions",
    "Number of technology solutions evaluated for feasibility",
    "Number of policies reviewed and updated based on evaluation outcomes",
    "Number of risks identified and mitigated in project implementation"
]

# Generate synthetic data
np.random.seed(42)
data_size = 300  # Number of samples
kmo_value_target_low = 0.3
kmo_value_target_high = 0.4

# Start with an SPD matrix
cov_matrix = make_spd_matrix(len(variables), random_state=42)
mean_vector = np.random.uniform(1, 10, len(variables))  # Mean values for variables
data = np.random.multivariate_normal(mean=mean_vector, cov=cov_matrix, size=data_size)
df = pd.DataFrame(data, columns=variables)

# Adjust data to lower correlations to meet KMO target
def adjust_kmo(df, target_low, target_high, max_steps=800):
    for i in range(max_steps):
        # Reduce correlations by adding small independent random noise per column
        for col in df.columns:
            df[col] = df[col] + np.random.normal(0, 0.1, df.shape[0])
        # Randomly shuffle a percentage of rows to break correlations further
        df = df.sample(frac=1).reset_index(drop=True)
        kmo_all, kmo_model = calculate_kmo(df)
        if target_low <= kmo_model < target_high:
            print(f"KMO target achieved: {kmo_model:.4f} at iteration {i+1}")
            return df, kmo_model
        if i % 100 == 0:
            print(f"Iteration {i}: KMO = {kmo_model:.4f}")
    print("KMO target not achieved within the maximum steps.")
    return df, kmo_model

# Apply KMO adjustment
df, kmo_model = adjust_kmo(df, kmo_value_target_low, kmo_value_target_high)

# Verify Bartlett's Test for Sphericity
bartlett_chi2, bartlett_p = calculate_bartlett_sphericity(df)

# Check both KMO and Bartlett's Test results
results = {
    "KMO Measure": kmo_model,
    "Bartlett's Test Chi-Square": bartlett_chi2,
    "Bartlett's Test p-value": bartlett_p
}

# Save data to a file
output_file = "medium_synthetic_data_factor_analysis.csv"
df.to_csv(output_file, index=False)

# Print results
print("\nFinal Results:")
print(results)
print(f"Synthetic data saved to {output_file}")

"""**High Income Data**"""

import numpy as np
import pandas as pd
from sklearn.datasets import make_spd_matrix
from factor_analyzer.factor_analyzer import calculate_kmo, calculate_bartlett_sphericity

# Variables as provided
variables = [
    "Reduction in local hardware costs due to cloud-based AI adoption",
    "Percentage reduction in required training data due to transfer learning",
    "Percentage reduction in resource consumption through sustainable practices",
    "Reduction in unplanned outages due to predictive maintenance",
    "Percentage reduction in household energy consumption after app usage",
    "Increase in energy efficiency due to grid optimization",
    "Reduction in average commute times for transit users",
    "Reduction in traffic congestion levels",
    "Increase in usage of bike-sharing and walking paths",
    "Reduction in fuel consumption for waste collection vehicles",
    "Increase in compost production efficiency",
    "Increase in green space coverage due to optimized placement",
    "Temperature reduction in targeted urban heat island areas",
    "Increase in water access points due to optimized kiosk placement",
    "Increase in water harvested per rainfall event",
    "Reduction in air pollution levels in identified hotspots",
    "Reduction in air pollution due to urban greening efforts",
    "Reduction in energy usage in buildings using smart thermostats",
    "Percentage increase in crop yield due to precision techniques",
    "Reduction in pest damage through AI-based detection",
    "Reduction in response time to disasters",
    "Increase in efficiency of disaster response resources",
    "Reduction in assessment time after disasters",
    "Reduction in crime rates in targeted areas",
    "Reduction in traffic accidents in managed areas",
    "Reduction in crowd-related incidents during events",
    "Reduction in resource wastage in healthcare systems",
    "Reduction in identified learning gaps through data-driven interventions",
    "Reduction in response time for disaster relief efforts",
    "Reduction in heat wave-related incidents in vulnerable areas",
    "Reduction in indoor air pollutant levels",
    "Increase in household income stability in vulnerable areas",
    "Increase in yield per square meter of urban farming",
    "Reduction in inequities identified through policy assessments",
    "Percentage increase in funds raised for local climate projects",
    "Reduction in environmental degradation metrics (CO2 levels)"
]

# Generate synthetic data
np.random.seed(42)
data_size = 300  # Number of samples
kmo_value_target_low = 0.3
kmo_value_target_high = 0.4

# Start with an SPD matrix
cov_matrix = make_spd_matrix(len(variables), random_state=42)
mean_vector = np.random.uniform(1, 10, len(variables))  # Mean values for variables
data = np.random.multivariate_normal(mean=mean_vector, cov=cov_matrix, size=data_size)
df = pd.DataFrame(data, columns=variables)

# Adjust data to lower correlations to meet KMO target
def adjust_kmo(df, target_low, target_high, max_steps=500):
    for i in range(max_steps):
        # Reduce correlations by adding small independent random noise per column
        for col in df.columns:
            df[col] = df[col] + np.random.normal(0, 0.1, df.shape[0])
        # Randomly shuffle a percentage of rows to break correlations further
        df = df.sample(frac=1).reset_index(drop=True)
        kmo_all, kmo_model = calculate_kmo(df)
        if target_low <= kmo_model < target_high:
            print(f"KMO target achieved: {kmo_model:.4f} at iteration {i+1}")
            return df, kmo_model
        if i % 100 == 0:
            print(f"Iteration {i}: KMO = {kmo_model:.4f}")
    print("KMO target not achieved within the maximum steps.")
    return df, kmo_model

# Apply KMO adjustment
df, kmo_model = adjust_kmo(df, kmo_value_target_low, kmo_value_target_high)

# Verify Bartlett's Test for Sphericity
bartlett_chi2, bartlett_p = calculate_bartlett_sphericity(df)

# Check both KMO and Bartlett's Test results
results = {
    "KMO Measure": kmo_model,
    "Bartlett's Test Chi-Square": bartlett_chi2,
    "Bartlett's Test p-value": bartlett_p
}

# Save data to a file
output_file = "high_synthetic_data_factor_analysis.csv"
df.to_csv(output_file, index=False)

# Print results
print("\nFinal Results:")
print(results)
print(f"Synthetic data saved to {output_file}")

"""**Low Income Data Synth.**"""

import numpy as np
import pandas as pd
from sklearn.datasets import make_spd_matrix
from factor_analyzer.factor_analyzer import calculate_kmo, calculate_bartlett_sphericity

# Variables as provided
variables = [
    "Average cost per IoT sensor unit deployed",
    "Percentage of total data collected via mobile applications",
    "Accuracy of NLP models in local language text processing",
    "Percentage of funds allocated based on participatory input",
    "Average internet speed in the target area",
    "Average latency in transmitting data across networks",
    "Total storage capacity available in local data centers",
    "Frequency of scheduled maintenance for systems",
    "Percentage of renewable energy usage within the microgrid",
    "Percentage of households with retrofitted smart meters installed",
    "Solar energy generation efficiency improvement",
    "Percentage of EVs integrated into the urban grid infrastructure",
    "User retention rate for waste sorting apps",
    "Accuracy of recycling material detection using AI",
    "Accuracy of flood risk prediction models",
    "Time taken to detect and fix leaks in water systems",
    "Accuracy of drought prediction models",
    "Percentage of households adopting clean cooking technologies",
    "Percentage improvement in ventilation efficiency for buildings",
    "Energy savings achieved through AI-driven behavior interventions",
    "Accuracy of ML models in solving specific tasks",
    "Language processing accuracy for low-resource languages",
    "Accuracy of object detection in real-world applications",
    "Accuracy of predicted vs actual crop yields",
    "Percentage of livestock health issues detected early using IoT",
    "Accuracy of market price prediction models",
    "Accuracy of outbreak predictions made by AI systems",
    "Percentage of accurate diagnoses made using AI systems",
    "Improvement in patient outcomes due to personalized plans",
    "Improvement in student learning outcomes due to personalization",
    "Percentage of population reached by early warning systems",
    "Time taken to complete needs assessment after a disaster",
    "Accuracy of disease prediction models",
    "Percentage of population with access to green spaces within 500 meters",
    "Percentage of food surplus redistributed through the platform",
    "Yield improvement for small-scale farmers using precision techniques",
    "Time saved in generating climate action reports",
    "Accuracy of finance need predictions for climate initiatives",
    "Success rate of funding proposals generated with AI assistance",
    "Percentage improvement in performance after iterative cycles",
    "Percentage of data compliant with privacy regulations",
    "Percentage of stakeholders represented in the design process",
    "Frequency of data collection intervals for monitoring",
    "Storage capacity utilized for project data",
    "Time taken from AI development to deployment in applications",
    "Frequency of scheduled maintenance activities",
    "Accuracy of sentiment analysis on public opinions regarding climate actions",
    "Accuracy of model predictions across different climate scenarios",
    "Level of community participation in different engagement models",
    "Percentage of stakeholders adopting proposed technologies",
    "Cost savings identified relative to implementation expenses"
]

# Generate synthetic data
np.random.seed(42)
data_size = 300  # Number of samples
kmo_value_target_low = 0.3
kmo_value_target_high = 0.4

# Start with an SPD matrix
cov_matrix = make_spd_matrix(len(variables), random_state=42)
mean_vector = np.random.uniform(1, 10, len(variables))  # Mean values for variables
data = np.random.multivariate_normal(mean=mean_vector, cov=cov_matrix, size=data_size)
df = pd.DataFrame(data, columns=variables)

# Adjust data to lower correlations to meet KMO target
def adjust_kmo(df, target_low, target_high, max_steps=6000):
    for i in range(max_steps):
        # Reduce correlations by adding small independent random noise per column
        for col in df.columns:
            df[col] = df[col] + np.random.normal(0, 0.1, df.shape[0])
        # Randomly shuffle a percentage of rows to break correlations further
        df = df.sample(frac=1).reset_index(drop=True)
        kmo_all, kmo_model = calculate_kmo(df)
        if target_low <= kmo_model < target_high:
            print(f"KMO target achieved: {kmo_model:.4f} at iteration {i+1}")
            return df, kmo_model
        if i % 100 == 0:
            print(f"Iteration {i}: KMO = {kmo_model:.4f}")
    print("KMO target not achieved within the maximum steps.")
    return df, kmo_model

# Apply KMO adjustment
df, kmo_model = adjust_kmo(df, kmo_value_target_low, kmo_value_target_high)

# Verify Bartlett's Test for Sphericity
bartlett_chi2, bartlett_p = calculate_bartlett_sphericity(df)

# Check both KMO and Bartlett's Test results
results = {
    "KMO Measure": kmo_model,
    "Bartlett's Test Chi-Square": bartlett_chi2,
    "Bartlett's Test p-value": bartlett_p
}

# Save data to a file
output_file = "low_synthetic_data_factor_analysis.csv"
df.to_csv(output_file, index=False)

# Print results
print("\nFinal Results:")
print(results)
print(f"Synthetic data saved to {output_file}")